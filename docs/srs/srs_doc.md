# Software Requirements Specification (SRS): BlazeLint

**Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 22.04.2025 <br>
**Verson** &nbsp;&nbsp;&nbsp;: 1.0.0 <br>
**Authors** &nbsp;: M. C. R. Mallawaarachchi, R. K. N. R. Ranasinghe, G. K. S. Pathum <br>

# 1. Introduction

## 1.1 Purpose

The purpose of this project is to develop a modular, extensible and highly pluggable code linter for Ballerina programming language, written in Rust. It will parse code into tokens and apply custom linting rules to identify common errors, bad practices, or violations of style guidelines.

## 1.2 Scope
This linter is designed to:
- Tokenize source code via a lexer
- Apply multiple linting rules (e.g., unknown tokens, bad declarations)
-  Report diagnostics such as warnings and errors
- Be extendable with new rules via a trait-based plugin architecture

## 1.3 Terminology

| Term       | Definition                                                   |
|------------|--------------------------------------------------------------|
| AST        | Abstract Syntax Tree                                         |
| Token      | A unit of code (keyword, symbol, identifier, etc.)           |
| Diagnostic | A warning or error reported by a rule                        |
| Rule       | A module that checks specific patterns or errors in the code |

# 2. Overall Description

## 2.1 Product Perspective
This tool is a standalone command-line application or library. It can be integrated with IDEs or used in CI pipelines.

## 2.2 Product Functions
1. **Lexical analysis** : Tokenizes source code
2. **Rule engine** : Applies a list of rules to token slices
3. **Diagnostics reporter** : Outputs structured issues
4. **Configuration** : Enables or disables specific rules

2.3 User Classes and Characteristics
Developers who want static code analysis

Educators teaching language semantics

Contributors adding new rules via trait implementations

2.4 Operating Environment
Rust compiler (edition 2021+)

Cross-platform (Linux, Windows, macOS)

CLI or integration via library

# 3. System Features 
## 3.1 Lexical Analyzer
The lexer is responsible for converting raw source code into a structured sequence of tokens, which are then used by the linting engine to apply rules.
- **Input**: Source code as a string.

- **Output**: `Vec<Token>` – a flat list of tokens.
- **Token Types**:

    - Identifier – variable or function names.

    - Keyword – language keywords like let, fn, etc.

    - Symbol – operators and punctuation (=, {, ;, etc.).

    - Literal – numbers, strings, etc.

    - Comment and Whitespace – optional, depending on usage.
## 3.2 Rule Checker
 Checks the sequence of tokens for potential coding issues by applying a set of predefined rules. Each rule examines the tokens and reports warnings or errors if it detects any violations.

 - **Input**: A slice of tokens `(&[Token])` generated by the lexer.
 - **Output**: List of lint warnings or errors (if any).
 - **Process**:

    - Applies each rule that implements the Rule trait.

    - Each rule inspects the tokens for specific patterns or violations.
## 3.3 Diagnostics Engine 
Collects and manages all the issues found during linting. Each diagnostic contains:
- A message describing the issue

- Severity level (warning or error)

- Location in the source code (line and column)

- The name of the rule that triggered it 

## 3.4 Configurable Rule Set
Allows customization of which rules are applied during linting. 

- Rules can be enabled or disabled using a config file (TOML or JSON)

- Users can optionally specify active rules through command-line arguments 
# 4. External Interface Requirements
## 4.1 User Interfaces


  

